{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (very) short primer on Bayesian vs. Frequentist regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPO Quantitative Methods Colloquium | 11 April 2017\n",
    "### Benjamin Skinner & Will Doyle\n",
    "#### [GitHub Repository](https://github.com/btskinner/lpoqmw/tree/master/apr_11_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## libraries and jupyter-specific options\n",
    "libs <- c('tidyverse','lme4','plotly','rstan','rstanarm','shinystan')\n",
    "suppressMessages(invisible(lapply(libs, require, character.only = TRUE)))\n",
    "options(width = 120, warn = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://imgs.xkcd.com/comics/frequentists_vs_bayesians.png)](https://xkcd.com/1132/)\n",
    "[![](https://imgs.xkcd.com/comics/seashell.png)](https://xkcd.com/1236/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Paper \n",
    "\n",
    "> Gill, Jeff and Christopher Witko (2013) \"[Bayesian Analytical Methods: A Methodological Prescription for Public Administration](https://academic.oup.com/jpart/article-abstract/23/2/457/1003493/Bayesian-Analytical-Methods-A-Methodological?redirectedFrom=fulltext).\" *Journal of Public Administration Research and Theory*, 23 (2), 457-494. doi: [10.1093/jopart/mus091](https://doi.org/10.1093/jopart/mus091)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to compute posterior distributions\n",
    "\n",
    "## Analytic (closed-form solutions)\n",
    "\n",
    "### EXAMPLE: Repeated binary trials (*i.e.,* coin flips)\n",
    "\n",
    "**Likelihood**: $p(X \\mid \\theta) \\sim $ [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "$$ p(X \\mid \\theta) = {n \\choose k}\\theta^k(1-\\theta)^{n-k}\\tag{$k$ successes in $n$ trials} $$\n",
    "\n",
    "**Prior**: $p(\\theta)\\sim $[Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n",
    "$$ p(\\theta) = \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1} \\tag{$\\alpha$ successes, $\\beta$ failures} $$\n",
    "\n",
    "**Posterior**: $p(\\theta \\mid X)\\sim $ [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n",
    "$$ p(\\theta \\mid X) \\propto {n \\choose k}\\theta^k(1-\\theta)^{n-k} \\times \\frac{1}{Beta(\\alpha,\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1} $$\n",
    "\n",
    "$$ p(\\theta \\mid X) \\propto \\theta^k(1-\\theta)^{n-k} \\times \\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1} $$\n",
    "\n",
    "$$ p(\\theta \\mid X) \\propto \\theta^{k + \\alpha - 1}(1-\\theta)^{n - k + \\beta - 1} $$\n",
    "\n",
    "$$ p(\\theta \\mid X) \\sim Beta(\\alpha + k, \\beta + n - k) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data (k = success; n = total trials)\n",
    "x <- c(rep(1, 250), rep(0, 150))\n",
    "k <- sum(x == 1)\n",
    "n <- length(x)\n",
    "\n",
    "## prior hyperparameters (a = success; b = failure)\n",
    "a <- 100\n",
    "b <- 100\n",
    "\n",
    "# ## prior, likelihood, posterior: random draws\n",
    "prior <- rbeta(n, a, b)\n",
    "likelihood <- rbinom(n, n, k/n)\n",
    "posterior <- rbeta(n, a + k, b + n - k)\n",
    "\n",
    "## put into data frame\n",
    "a_bayes <- data.frame(theta = c(prior, likelihood / n, posterior),\n",
    "                     distribution = c(rep('prior', length(prior)),\n",
    "                                     rep('likelihood', length(likelihood)),\n",
    "                                     rep('posterior', length(posterior)))) %>%\n",
    "            mutate(distribution = as.factor(distribution))\n",
    "\n",
    "## plot\n",
    "ggplot(a_bayes, aes(x = theta, fill = distribution)) + \n",
    "    geom_density(alpha = 0.25, adjust = 2) +\n",
    "    ylab('') +\n",
    "    xlab(expression(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic computation: pros and cons\n",
    "\n",
    "**Pros**\n",
    "1. Sometimes very easy to compute\n",
    "2. Gives the clearest description of the posterior\n",
    "\n",
    "**Cons**\n",
    "1. Algebra + [knowledge of conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior)\n",
    "2. Almost never available for interesting problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo (MCMC) Sampling\n",
    "\n",
    "For most interesting problems, there isn't an analytic or closed form solution to the posterior distribution. Or it's a pain to compute. In these situations one can try to sample directly from the posterior distribution by (1) choosing a computational algorithm and (2) relying on [Markov Chain Monte Carlo (MCMC) theory](http://www.mcmchandbook.net/HandbookChapter1.pdf) to make inferences. MCMC theory is an entire topic unto itself, so we won't delve into it today. Suffice to say that it is what permits us to feel confident that under certain conditions, our samples represent a good/useful approximation of the true posterior.\n",
    "\n",
    "### MCMC alogorithms in terms of Bayesian analysis (50k feet view of the procedure)\n",
    "1. Decide upon method for choosing a value of an unknown parameter ($\\beta$ in a regression, for example)\n",
    "2. Have rule for deciding whether value is a good choice and store (or not) accordingly\n",
    "\n",
    "#### Procedure\n",
    "\n",
    "To produce $S$ samples from the posterior:\n",
    "    \n",
    "    1. Initialize all unknowns (e.g., regression coefficients) with some starting values  \n",
    "    while (s < S):\n",
    "        2. Choose new parameter\n",
    "        3. Implement decision rule:\n",
    "            1. Keep new parameter if rule says it's good\n",
    "            2. Drop new parameter if rule says it's bad and keep old one instead\n",
    "        4. Store value based on decision\n",
    "\n",
    "As always, the devil is in the details. Specifically:  \n",
    "1. *What's a good way to generate new values for parameters?*  \n",
    "2. *What's a good rule for deciding whether to keep the new parameter?*\n",
    "\n",
    "## MCMC samplers and swimming pools\n",
    "\n",
    "There are many, many methods for sampling from the posterior. The following three are the most common. We won't go into the math behind each, but those who are interested can check out the first chapter of the [MCMC Handbook](http://www.mcmchandbook.net/HandbookChapter1.pdf). Conceptualizing a multidimensional distribution is difficult, so for purposes of illustration, we'll limit ourselves to a two-dimensional distribution that has a single maximum point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## create data that are distributed bivariate normal\n",
    "mu <- c(0,0)\n",
    "Sigma <- matrix(c(1,0,0,1),2,2)\n",
    "beta <- MASS::mvrnorm(200, mu, Sigma)\n",
    "dens <- MASS::kde2d(beta[,1], beta[,2], n = nrow(beta))\n",
    "\n",
    "## interactive plot\n",
    "axx <- list(title = 'beta_1')\n",
    "axy <- list(title = 'beta_2')\n",
    "axz <- list(title = 'Joint density')\n",
    "p <- plot_ly(x = dens$x, y = dens$y, z = dens$z) %>% \n",
    "    add_surface() %>%\n",
    "    layout(scene = list(xaxis = axx, yaxis = axy, zaxis = axz))\n",
    "embed_notebook(p, height = '650px', file = 'figures/bivariate.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint distribution of the parameters, $ \\beta_1 $ and $ \\beta_2 $, is most likely at the peak (yellow). For purpose of visualizing our sampler, let's flip the distribution upside down. Now the joint likelihood is greatest at the bottom: a swimming pool!\n",
    "\n",
    "The goal of the sample is describe the swimming pool by moving around the pool and taking snapshots along the way. We are interested in the deepest part of the pool because that's the point at which our unknown parameters are most likely. But since we're Bayesians now, we're also interested in the parts that aren't quite as likely. (Just because are they less likely doesn't mean they *won't* ever occur.) We want to describe the full distribution...the full pool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampler\n",
    "\n",
    "The [Gibbs Sampler](https://en.wikipedia.org/wiki/Gibbs_sampling) is a compromise between an analytic and a computational approach. Think of a posterior with multiple unknowns, for example, a normal linear regression where $\\beta$ and the regression variance term, $\\sigma^2$, are unknown. The full joint posterior, $ p(\\beta, \\sigma^2 \\mid X) $, may be intractable or difficult to solve. On the other hand, the marginal posterior distributions of the terms, $ p(\\beta \\mid X, \\sigma^2) $ and $ p(\\sigma^2 \\mid X, \\beta) $, may be more tractable.\n",
    "\n",
    "The Gibbs sampling algorithm therefore uses a bit of algebra to rearrange the full posterior into a set of marginal posteriors, one for each unknown or group of unknowns. The algorithm then follows this procedure:\n",
    "\n",
    "```\n",
    "1. initialize all parameters\n",
    "for (s < S):\n",
    "    2. update first unknown\n",
    "    3. update second unknown\n",
    "    ...\n",
    "    4. update last unknown\n",
    "    5. store all updates\n",
    "```\n",
    "\n",
    "Because the marginal posteriors are solveable, all new parameter choices are selected when updating.\n",
    "\n",
    "#### Gibbs and the pool\n",
    "\n",
    "Thinking about our swimming pool, it works like this:\n",
    "\n",
    "1. Start at some arbitraty point in your pool\n",
    "2. Pick one dimension of the pool (width- or length-wise)\n",
    "3. Draw a straight line along that dimension, one end to the other\n",
    "4. Move to the point on that line that is at the lowest point of the pool\n",
    "5. Now choose the other dimension (perpendicular to the first) and draw a straight line along it\n",
    "6. Move again (perpendicular to the first move) until you reach the lowest point along that line\n",
    "7. Repeat until you stop moving that much or for an arbitrary number of steps\n",
    "\n",
    "Eventually, no matter where you start, you should reach the deepest point. Under some conditions, the Gibbs sampler can find the posterior maximum (deepest part of the pool) pretty quickly. It may do so, however, at the expense of really exploring the full posterior/pool.\n",
    "\n",
    "### Metropolis-Hastings\n",
    "\n",
    "The [Metropolis-Hastings](https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm) sampler doesn't try to compute marginal posteriors. Instead, it selects new parameters from a proportional candidate distribution, compares a function of these parameters to a function of the old parameters, and accepts/rejects based on a probabilistic decision rule. The steps for MH in a regression framework are:\n",
    "\n",
    "```\n",
    "1. initialize all parameters\n",
    "for (s < S):\n",
    "    2. choose new parameters from candidate distribution\n",
    "    3. compute the log likelihood (ll) of the data using these parameters\n",
    "    4. compute ratio: ll_new / ll_old = alpha\n",
    "    5. accept with probability: min(1, alpha)\n",
    "\n",
    "```\n",
    "\n",
    "The MH sampler will always accept new values that improve the fit (increase the log-likelihood). It won't necessarily reject those that reduce the log-likelihood, however. If they are close ($\\alpha = .98$), they are quite likely to be accepted. On rare occasions, they will be accepted even when they are not that great. This feature allows the sampler to \"jump around\" and (hopefully) not become stuck in local maxima.\n",
    "\n",
    "#### MH and the pool\n",
    "\n",
    "Returning to the pool, Metropolis-Hastings works like this:\n",
    "\n",
    "1. Pick some inital values for all your unknowns\n",
    "2. Pick another pool that you think has a similar shape* to your pool that:  \n",
    "    1. covers your pool (is bigger than)  \n",
    "    2. you can describe well (you know what it looks like)\n",
    "3. Move the point in the target pool that your random parameters give you\n",
    "4. Compare the depth of this point to the depth at the old point  \n",
    "    1. if deeper record this point  \n",
    "    2. if shallower, get a random number between 1 and 100 and divide by 100\n",
    "        1. if number is < ratio of new depth over old depth, record new value\n",
    "        2. if number is > ratio of new depth over old depth, record old value again\n",
    "5. Repeat for a determined number of steps\n",
    "\n",
    "\\* The shape of the proposal pool doesn't have to match the target pool exactly. If it can be transformed to fit the shape of the proposal pool, that will work, too. \n",
    "\n",
    "Unlike the Gibbs sampler, the Metropolis-Hastings sampler is more likely to explore more of the pool since it jumps around. What if the shallow end of the pool isn't smoothly sloping but rather has an indentation? The Gibbs sample could become stuck here, but the MH can jump out. The MH sampler requires, however, that the proposal pool fully \"contain\" the target. If it doesn't, there will be some places of the target pool that you'll never visit because they aren't part of the proposal pool. The MH sampler also requires some tuning so that it gives good proposal values. Too many bad ones (outside the target pool), will mean that you don't move much. Too many that are accepted and it will be very slow, taking random baby steps around the pool.\n",
    "\n",
    "### Hamiltonian Monte Carlo\n",
    "\n",
    "The Hamilton Monte Carlo sampler is the hot new sampler. It uses [Hamiltonian mechanics](https://en.wikipedia.org/wiki/Hamiltonian_mechanics) to simulate a particle moving around the posterior parameter space. The particle is given both potential and kinetic energy. Based on the gradient (shape) of the posterior, the particle is given direction. \n",
    "\n",
    "Imagine a hockey puck given a shove in a swimming pool. It will move around the pool, going up and down the sides. It may land in the shallow end quite often, but over time, it will more often end up in the deep end. Provided the algorithm can be tuned well (so the puck doesn't go flying outside of the pool), the HMC sampler can be very efficient at exploring the posterior parameter space.\n",
    "\n",
    "Stan uses a variant of the HMC, the [No-U-Turn Sampler (NUTS)](http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf). Because the HMC particle can double back on itself, it's not quite as efficient as it could be (imagine shoving the hockey puck and after moving all over the pool, it landed two inches from your hand). NUTS uses some additional coding to make sure that the HMC doesn't double back: no U turns!\n",
    "\n",
    "## Enough words...let's see them in action!\n",
    "\n",
    "[Visualizations of various MCMC algorithms](http://chi-feng.github.io/mcmc-demo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding examples: Bayesian vs. frequentist regression\n",
    "\n",
    "## Data\n",
    "\n",
    "Data come from the Kindergarten year of the Tennessee STAR class reduction experiment. The data can be downloaded from the [Harvard Dataverse archive](https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/10766). The script to make the reduced data set used in these examples, `clean_data.R`, is in the [Github repository](https://github.com/btskinner/lpoqmw/tree/master/apr_11_2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data\n",
    "df <- read_csv('tn_star_k.csv', \n",
    "               col_types = cols(.default = col_integer(), \n",
    "                                math_std = col_double(),\n",
    "                                read_std = col_double()))\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codebook\n",
    "\n",
    "|Variable|Description|\n",
    "|:--|:--|\n",
    "|studnid | Unique student ID|\n",
    "|female| == 1 if student is female|\n",
    "|black| == 1 if student is Black|\n",
    "|asian| == 1 if student is Asian|\n",
    "|hispc| == 1 if student is Hispanic|\n",
    "|natam| == 1 if student is Native American|\n",
    "|orace| == 1 if student is another Race/ethnicity|\n",
    "|t\\_smc| == 1 if treatment is a small class|\n",
    "|t\\_rgc| == 1 if treatment is a regular class|\n",
    "|t\\_rga| == 1 if treatment is a regular class with a teacher's aide|\n",
    "|incit| == 1 if student lives in inner city area|\n",
    "|subur| == 1 if student lives in suburban area|\n",
    "|urban| == 1 if student lives in urban area|\n",
    "|rural| == 1 if student lives in rural area|\n",
    "|frpr| == 1 if student receives free or reduced price lunch|\n",
    "|spced| == 1 if student receives special education services|\n",
    "|math| math test score|\n",
    "|read| reading test score|\n",
    "|math\\_std| standardized math test score|\n",
    "|read\\_std| standardized reading test score|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Level Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Formula **\n",
    "$$ y_i = \\beta_0 + \\beta_{smc} \\cdot T_i^{small\\,class} + \\beta_{aid} \\cdot T_i^{aide} + X_i \\beta_k + \\varepsilon_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## frequentist linear model\n",
    "freq_lm <- lm(math_std ~ t_smc + t_rga + female + black + asian\n",
    "              + hispc + natam + orace + incit + subur + urban\n",
    "              + frpl + spced,\n",
    "              data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print\n",
    "summary(freq_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show Stan model\n",
    "writeLines(readLines('normal.stan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## compile Stan model\n",
    "norm_mod <- stan_model(file = 'normal.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## set up Stan data in list\n",
    "y <- df$math_std\n",
    "x <- df %>%\n",
    "    select(t_smc, t_rga, female, black, asian,\n",
    "           hispc, natam, orace, incit, subur, urban,\n",
    "           frpl, spced) %>%\n",
    "    as.matrix(.)\n",
    "N <- nrow(x)\n",
    "K <- ncol(x)\n",
    "\n",
    "stan_df <- list('y' = y, 'x' = x, 'N' = N, 'K' = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Bayesian linear model using rstan\n",
    "bayes_lm_1 <- sampling(norm_mod,\n",
    "                       data = stan_df,\n",
    "                       iter = 1000,\n",
    "                       chains = 3,\n",
    "                       cores = 3,\n",
    "                       pars = c('alpha','beta','sigma','lp__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## print\n",
    "print(bayes_lm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Bayesian linear model using rstanarm\n",
    "bayes_lm_2 <- stan_glm(math_std ~ t_smc + t_rga + female + black + asian\n",
    "                       + hispc + natam + orace + incit + subur + urban\n",
    "                       + frpl + spced,\n",
    "                       data = df,\n",
    "                       family = gaussian(link = 'identity'),\n",
    "                       chains = 3,\n",
    "                       cores = 3,\n",
    "                       iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print\n",
    "summary(bayes_lm_2, digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## check convergence\n",
    "# launch_shinystan(bayes_lm_1, quiet = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying intercept models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Formula **\n",
    "\\begin{align} \n",
    "y_i &= \\beta_j + \\beta_{smc} \\cdot T_i^{small\\,class} + \\beta_{aid} \\cdot T_i^{aide} + X_i \\beta_k + \\varepsilon_i \\\\\n",
    "\\beta_j &\\sim Normal(\\mu, \\sigma_j)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## varying intercept model\n",
    "freq_vi <- lmer(math_std ~ -1 + t_smc + t_rga + female + black + asian\n",
    "                + hispc + natam + orace + incit + subur + urban\n",
    "                + frpl + spced + (1 | school),\n",
    "                data = df)\n",
    "summary(freq_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show Stan model\n",
    "writeLines(readLines('vi_normal.stan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## compile Stan model\n",
    "vi_norm_mod <- stan_model(file = 'vi_normal.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add information for school-level varying intercepts\n",
    "school <- df$school\n",
    "J <- length(unique(school))\n",
    "\n",
    "stan_df <- list('y' = y, 'x' = x, 'N' = N, 'K' = K, 'J' = J, 'school' = school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Bayesian varying intercept model using rstan\n",
    "bayes_vi_1 <- sampling(vi_norm_mod,\n",
    "                       data = stan_df,\n",
    "                       iter = 2000,\n",
    "                       chains = 3,\n",
    "                       cores = 3,\n",
    "                       pars = c('alpha_mu','alpha','beta',\n",
    "                                'alpha_sigma','sigma','lp__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print results\n",
    "print(bayes_vi_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Bayesian varying intercept model using rstanarm\n",
    "bayes_vi_2 <- stan_glmer(math_std ~ -1 + t_smc + t_rga + female + black + asian\n",
    "                         + hispc + natam + orace + incit + subur + urban\n",
    "                         + frpl + spced + (1 | school),\n",
    "                         data = df,\n",
    "                         family = gaussian(link = 'identity'),\n",
    "                         chains = 3,\n",
    "                         cores = 3,\n",
    "                         iter = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## print results\n",
    "summary(bayes_vi_2, digits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## check convergence\n",
    "# launch_shinystan(bayes_vi_1, quiet = TRUE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
